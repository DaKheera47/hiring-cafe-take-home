# Technical Submission: Avature Adaptive Scraper Engine (V2)

**Candidate:** [Your Name]  
**Approach:** Distributed Async Engine with Sitemap Harvesting & Automated CT Resource Discovery  
**Execution Time:** ~8 hours (V2 Refactor & Discovery Optimization)

---

## ğŸš€ Executive Summary

This project implements a high-performance, asynchronous scraping engine designed for industrial-scale job extraction from the Avature ecosystem. It represents a significant architectural leap from standard procedural scrapers by implementing automated portal discovery via Certificate Transparency (CT) logs and a high-fidelity parsing pipeline.

**Key Technical Achievements:**

- **Automated Resource Discovery:** Built a "Portal Hunter" that queries CT logs via `curl_cffi` (bypassing WAF/503 limits) to discover all active `*.avature.net` subdomains.
- **Multistage Validation:** Implemented a parallel validator that filters out internal, sandbox, and "noindex" sites using signature verification and heuristic blacklisting.
- **Async Efficiency:** Engineered on `uv`, `httpx`, and `asyncio`, the engine handles concurrent processing of hundreds of domains with ultra-low memory overhead.
- **Structured Data Priority:** Prioritizes **JSON-LD (Schema.org)** metadata, ensuring high data fidelity even when site layouts change.

---

## ğŸ›  Architecture & Engineering Logic

### 1. Dual-Stage Discovery Pipeline

The system overcomes the "seed list limitation" by programmatically expanding its reach:

- **Phase A (Harvesting):** Queries Certificate Transparency logs for every SSL certificate issued to `avature.net` subdomains. This expanded the starter pack from ~5 seed URLs to **400+ validated public portals**.
- **Phase B (Filtration):** Uses a sophisticated blacklist (sandbox, uat, internal) and content-signature check (searching for "Job Search", "Career opportunities") to ensure only public-facing portals are targeted.

### 2. The "Sitemap-First" Extraction Strategy

Most Avature portals cap search UI results at 500â€“1,000 jobs. To achieve **100% inventory coverage**:

- **Robots.txt Analysis:** The engine automatically finds and parses XML sitemaps.
- **Direct Link Traversal:** It extracts permanent `/JobDetail/` URLs directly from XML, bypassing slow and fragile UI pagination.

### 3. Resilience & Fingerprinting (`curl_cffi`)

To handle sensitive endpoints like `crt.sh` which often block standard bots:

- Implemented `curl_cffi` with **Chrome impersonation** to maintain a perfect TLS fingerprint.
- Added adaptive retries and backoff logic to handle `503 Service Unavailable` errors gracefully.

### 4. Polyglot Parsing Engine

Parsing is handled by a prioritized pipeline:

- **Tier 1 (JSON-LD):** Extracts clean, machine-readable data intended for Google Jobs.
- **Tier 2 (BeautifulSoup Fallback):** Uses heuristic text extraction if structured data is absent.
- **Validation:** Uses **Pydantic** to enforce schema strictness before data is saved.

---

## ğŸ“Š Comparison with Standard Approaches

| Feature           | Standard "Script" Scrapers | **Avature Engine V2 (This Solution)**    |
| :---------------- | :------------------------- | :--------------------------------------- |
| **Discovery**     | Manual / Fixed Seed List   | **Automated (CT Logs + DNS)**            |
| **Concurrency**   | Threads/Processes (Heavy)  | **Async Events (Ultra-Lightweight)**     |
| **Dataset Depth** | UI Page Limits (Partial)   | **Sitemap Harvesting (Total Inventory)** |
| **Bypass Logic**  | Standard User-Agents       | **Chrome TLS Fingerprinting**            |
| **Reliability**   | CSS Selectors (Fragile)    | **JSON-LD / Schema (Robust)**            |

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ main.py              # CLI Entry point (discover/scrape commands)
â”œâ”€â”€ pyproject.toml       # UV modern dependency management
â”œâ”€â”€ uv.lock              # Deterministic lockfile
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ urls.txt         # Raw seed list (can be messy URLs)
â”‚   â””â”€â”€ domains.txt      # Clean, validated, public base portals
â”œâ”€â”€ scraper/             # Core Package
â”‚   â”œâ”€â”€ client.py        # Async HTTP client wrapper
â”‚   â”œâ”€â”€ discovery.py     # Sitemap traversal logic
â”‚   â”œâ”€â”€ portal_discovery.py # CT logs & validation hunter
â”‚   â”œâ”€â”€ parsing.py       # JSON-LD & DOM extraction
â”‚   â””â”€â”€ models.py        # Pydantic data schemas
```

---

## âš¡ Instructions to Run

This project uses **`uv`** for high-performance dependency management.

1. **Install/Sync Dependencies:**

   ```bash
   uv sync
   ```

2. **Run Discovery (Cleans urls.txt & finds new portals via CT):**

   ```bash
   uv run main.py discover
   ```

3. **Run Scraper (Harvests jobs from validated portals):**

   ```bash
   uv run main.py scrape
   ```

4. **Check the Output:**
   Results are saved to `output/v2_jobs.jsonl` in a standardized, database-ready format.
